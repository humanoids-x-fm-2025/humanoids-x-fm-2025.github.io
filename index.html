<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>HumanoidsxFoundationModels</title>
    <link rel="stylesheet" href="styles.css" />
    <link href="https://fonts.googleapis.com/css2?family=Outfit:wght@400;600;800&display=swap" rel="stylesheet">

    <!-- Open Graph / Social Media Preview -->
    <meta property="og:title" content="Humanoids 2025 Workshop" />
    <meta property="og:description"
        content="Bridging Humanoid Robotics and Foundation Models: Embodied Intelligence and AI Integration ¬∑ October 2, 2025 ¬∑ COEX, Seoul" />
    <meta property="og:image" content="https://humanoids-x-fm-2025.github.io/assets/sticker_nobg.png" />
    <meta property="og:url" content="https://humanoids-x-fm-2025.github.io/" />
    <meta property="og:type" content="website" />

    <!-- Twitter Card (optional but recommended) -->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Humanoids 2025 Workshop" />
    <meta name="twitter:description"
        content="Join us in Seoul for a full-day workshop on Humanoid Robotics and Foundation Models." />
    <meta name="twitter:image" content="https://humanoids-x-fm-2025.github.io/assets/sticker_nobg.png" />

    <link rel="icon" type="image/png" href="assets/favicon-96x96.png" sizes="96x96" />
    <link rel="icon" type="image/svg+xml" href="assets/favicon.svg" />
    <link rel="shortcut icon" href="assets/favicon.ico" />
    <link rel="apple-touch-icon" sizes="180x180" href="assets/apple-touch-icon.png" />
    <link rel="manifest" href="assets/site.webmanifest" />
</head>

<body>
    <header>
        <nav class="container">
            <div class="logo">
                <img src="./assets/sticker_nobg.png" alt="Humanoids 2025 Logo" class="logo-img" />
                Bridging Humanoid Robotics and Foundation Models Workshop
            </div>
            <ul class="nav-links">
                <li><a href="#about">Objective and Scope</a></li>
                <li><a href="#speakers">Speakers</a></li>
                <li><a href="#organizers">Organizers</a></li>
                <li><a href="#program">Program</a></li>
                <li><a href="#cfp">Accepted Posters</a></li>
                <li><a href="#contact">Contact</a></li>
            </ul>
        </nav>
    </header>

    <section class="hero">
        <div class="overlay"></div>
        <img src="./assets/logo_landscape.png" alt="Humanoid Seoul" class="hero-bg" />
        <div class="hero-text container">
            <h1>Bridging Humanoid Robotics and Foundation Models: Embodied Intelligence and AI Integration</h1>
            <p>October 2, 2025 ¬∑ COEX, Seoul</p>
            <p style="font-size:0.9rem;">Full-day Workshop</p>
        </div>
    </section>

    <main class="container">
        <section id="about" class="section">
            <h2>Objective and Scope</h2>
            <p> The integration of foundation models, such as Large Language Models (LLMs), Vision-Language Models
                (VLMs), Vision-Language-Action Models (VLAMs), and other multimodal architectures, has the potential to
                fundamentally reshape how humanoid robots perceive, reason, act, and interact in the world.
                Until recently, general-purpose humanoid robots were constrained by the limited scalability of
                traditional planning, control, and perception pipelines. These robots, designed to understand
                instructions, adapt to novel environments, and engage in human-like interactions, often struggled to
                meet the flexibility required for general use. Foundation models offer a major step forward by enabling
                abstract reasoning, grounding through multi-modal inputs, and fast generalization across tasks and
                domains.
                However, integrating these models into humanoid robots presents open challenges. Questions remain about
                grounding abstract knowledge into sensorimotor data, combining model-based control with learned
                representations, ensuring safety and interpretability, and enabling embodied models to interact socially
                and linguistically with humans.
                Humanoid platforms, ranging from full-body anthropomorphic robots to more abstract bimanual systems, are
                especially well-suited for this line of research. Their structural similarity to humans and versatility
                make them ideal for exploring foundation model grounding, physical reasoning, and social interaction in
                human environments.
                This workshop is designed to promote collaboration between academic researchers and industry
                practitioners. The program will feature keynote talks, a cross-sector panel discussion, and a call for
                contributed presentations, with a particular focus on emerging research at the intersection of humanoid
                robotics and foundation models.
                By fostering this convergence, the workshop aims to build a roadmap toward generalizable, human-aligned,
                and socially capable humanoid systems powered by foundation models.
            </p>
            <h4>Topics</h4>

            <ul class="topics-list">
                <li><strong>Models for Humanoid Robotics:</strong> Use of large pre-trained language and vision-language
                    models for planning, decision-making, and control in humanoid systems</li>
                <li><strong>Multimodal Perception and World Modeling:</strong> Leveraging VLMs for scene understanding,
                    object grounding,
                    spatial awareness, and physical commonsense reasoning</li>
                <li><strong>Policy Learning with Foundation Models:</strong> Language-guided reinforcement and imitation
                    learning, as
                    well as model-based control augmented by LLMs</li>
                <li><strong>Human-Robot Interaction and Social Intelligence:</strong> Social reasoning, linguistic
                    interaction, and
                    collaborative behavior in human-centered environments</li>
                <li><strong>Safety, Interpretability, and Human Alignment:</strong> Ensuring safe, transparent, and
                    aligned behavior in
                    LLM- or VLM-driven humanoid systems</li>
                <li><strong>Simulation, Evaluation, and Benchmarks:</strong> Tools and protocols for benchmarking
                    embodied foundation
                    models in humanoid tasks, including real-world and simulated settings</li>
                <li><strong>Applications in Complex Environments:</strong> Household assistance, healthcare, industrial
                    tasks, mobile
                    manipulation, and human-facing applications involving physical and social complexity</li>
            </ul>

        </section>

        <section id="speakers" class="section">
            <h2>Speakers</h2>
            <div class="profile-grid-speakers">

                <a href="https://www.ias.tu-darmstadt.de/Team/JanPeters" target="_blank" class="card-link">
                    <div class="card">
                        <div class="profile">
                            <div class="circle-img" style="background-image: url('assets/jp.jpg');"></div>
                            <p><strong>Jan<br>Peters</strong><br>TU Darmstadt | DFKI</p>
                        </div>
                    </div>
                </a>

                <a href="https://keerthanapg.com/about/" target="_blank" class="card-link">
                    <div class="card">
                        <div class="profile">
                            <div class="circle-img" style="background-image: url('assets/kg.jpeg');"></div>
                            <p><strong>Keerthana<br>Gopalakrishnan</strong><br>Google Deepmind</p>
                        </div>
                    </div>
                </a>

                <!-- <a href="https://ryanjulian.me/" target="_blank" class="card-link">
                    <div class="card">
                        <div class="profile">
                            <div class="circle-img" style="background-image: url('assets/rj.jpeg');"></div>
                            <p><strong>Ryan<br>Julian</strong><br>Nvidia GEAR</p>
                        </div>
                    </div>
                </a> -->

                <a href="https://haraduka.github.io/" target="_blank" class="card-link">
                    <div class="card">
                        <div class="profile">
                            <div class="circle-img" style="background-image: url('assets/kw.png');"></div>
                            <p><strong>Kento<br>Kawaharazuka</strong><br>The University of Tokyo</p>
                        </div>
                    </div>
                </a>

                <a href="https://xiaolonw.github.io/" target="_blank" class="card-link">
                    <div class="card">
                        <div class="profile">
                            <div class="circle-img" style="background-image: url('assets/xw.jpg');"></div>
                            <p><strong>Xiaolong<br>Wang</strong><br>UC San Diego</p>
                        </div>
                    </div>
                </a>

                <a href="https://rogerqi.github.io/" target="_blank" class="card-link">
                    <div class="card">
                        <div class="profile">
                            <div class="circle-img" style="background-image: url('assets/rq.jpeg');"></div>
                            <p><strong>Ri-Zhao (Roger)<br>Qiu</strong><br>UC San Diego</p>
                        </div>
                    </div>
                </a>

                <a href="https://joeljang.github.io/" target="_blank" class="card-link">
                    <div class="card">
                        <div class="profile">
                            <div class="circle-img" style="background-image: url('assets/jj.jpg');"></div>
                            <p><strong>Joel<br>Jang</strong><br>Nvidia GEAR Lab</p>
                        </div>
                    </div>
                </a>

                <a href="https://rudolf.intuitive-robots.net/" target="_blank" class="card-link">
                    <div class="card">
                        <div class="profile">
                            <div class="circle-img" style="background-image: url('assets/rl.jpg');"></div>
                            <p><strong>Rudolf<br>Lioutikov</strong><br>KIT</p>
                        </div>
                    </div>
                </a>

            </div>
        </section>


        <section id="organizers" class="section">
            <h2>Organizers</h2>
            <div class="profile-grid-organizers">

                <a href="https://dtotsila.github.io/" target="_blank" class="card-link">
                    <div class="card">
                        <div class="profile">
                            <div class="circle-img" style="background-image: url('assets/dt.jpg');"></div>
                            <p><strong>Dionis <br> Totsila</strong><br>INRIA</p>
                        </div>
                    </div>
                </a>

                <a href="https://members.loria.fr/SIvaldi/" target="_blank" class="card-link">
                    <div class="card">
                        <div class="profile">
                            <div class="circle-img" style="background-image: url('assets/si.jpg');"></div>
                            <p><strong>Serena <br> Ivaldi</strong><br>INRIA</p>
                        </div>
                    </div>
                </a>

                <a href="https://h2t.iar.kit.edu/english/21_2372.php" target="_blank" class="card-link">
                    <div class="card">
                        <div class="profile">
                            <div class="circle-img" style="background-image: url('assets/ta.JPG');"></div>
                            <p><strong>Tamim <br> Asfour</strong><br>KIT</p>
                        </div>
                    </div>
                </a>

                <a href="https://keerthanapg.com/about/" target="_blank" class="card-link">
                    <div class="card">
                        <div class="profile">
                            <div class="circle-img" style="background-image: url('assets/kg.jpeg');"></div>
                            <p><strong>Keerthana <br> Gopalakrishnan</strong><br>Google Deepmind</p>
                        </div>
                    </div>
                </a>

                <a href="https://ryanjulian.me/" target="_blank" class="card-link">
                    <div class="card">
                        <div class="profile">
                            <div class="circle-img" style="background-image: url('assets/rj.jpeg');"></div>
                            <p><strong>Ryan<br>Julian</strong><br>NVIDIA GEAR</p>
                        </div>
                    </div>
                </a>

                <a href="https://leph.io/quentinrouxel/" target="_blank" class="card-link">
                    <div class="card">
                        <div class="profile">
                            <div class="circle-img" style="background-image: url('assets/sticker_nobg.png');"></div>
                            <p><strong>Quentin<br>Rouxel</strong><br>CUHK (Hong Kong)</p>
                        </div>
                    </div>
                </a>

            </div>
        </section>



        <section id="program" class="section">
            <h2>Preliminary Program</h2>
            <table class="program-table">
                <tr>
                    <td class="sched_time">09:00 - 09:30</td>
                    <td class="sched_speaker">
                        <strong>Welcome & Introduction</strong>
                    </td>
                </tr>
                <tr>
                    <td class="sched_time">09:30 - 10:00</td>
                    <td class="sched_speaker">
                        <strong>Keerthana Gopalakrishnan</strong>
                    </td>
                </tr>
                <tr>
                    <td class="sched_time">10:00 - 10:30</td>
                    <td class="sched_speaker">
                        <strong>Booster Presentations of Accepted Posters</strong>
                    </td>
                </tr>
                <tr>
                    <td class="sched_time">10:30 - 11:00</td>
                    <td class="sched_speaker">
                        <strong>‚òï Coffee Break</strong>
                    </td>
                </tr>
                <tr>
                    <td class="sched_time">11:00 - 11:30</td>
                    <td class="sched_speaker">
                        <strong>Speaker 2</strong>

                    </td>
                </tr>
                <tr>
                    <td class="sched_time">11:30 - 12:00</td>
                    <td class="sched_speaker">
                        <strong>Kento Kawaharazuka | Foundation Model-based Recognition and Planning for Humanoid Robots
                        </strong>

                    </td>
                </tr>
                <tr>
                    <td class="sched_time">12:00 - 14:00</td>
                    <td class="sched_speaker">
                        <strong>üçΩÔ∏è Lunch break</strong>
                    </td>
                </tr>
                <tr>
                    <td class="sched_time">14:00 - 14:30</td>
                    <td class="sched_speaker">
                        <strong>Joel Jang | Why do we need Humanoid Robots? Perspective with video world models</strong>

                    </td>
                </tr>
                <tr>
                    <td class="sched_time">14:30 - 15:00</td>
                    <td class="sched_speaker">
                        <strong>Speaker 5</strong>

                    </td>
                </tr>
                <tr>
                    <td class="sched_time">15:00 - 15:30</td>
                    <td class="sched_speaker">
                        <strong>‚òï Coffee Break</strong>
                    </td>
                </tr>
                <tr>
                    <td class="sched_time">15:30 - 16:00</td>
                    <td class="sched_speaker">
                        <strong>Speaker 6</strong>

                    </td>
                </tr>
                <tr>
                    <td class="sched_time">16:00 - 16:45</td>
                    <td class="sched_speaker">
                        <strong>Discussion Panel</strong>
                    </td>
                </tr>
                <tr>
                    <td class="sched_time">16:45 - 17:00</td>
                    <td class="sched_speaker">
                        <strong>Closing Remarks</strong>
                    </td>
                </tr>
                <tr>
                    <td class="sched_time">17:00</td>
                    <td class="sched_speaker">
                        <strong>End</strong>
                    </td>
                </tr>
            </table>
        </section>

        <section id="cfp" class="section">
            <h2>Accepted Posters</h2>

            <div class="poster-container">

                <div class="poster">
                    <div class="poster-title" onclick="toggleAbstract(0)">
                        Can We Detect Failures Without Failure Data? Uncertainty-Aware Runtime Failure Detection for
                        Imitation Learning Policies
                    </div>
                    <div class="poster-authors">
                        Chen Xu<sup>1</sup>, Tony Khuong Nguyen<sup>1</sup>, Emma Dixon<sup>1</sup>, Christopher Rodriguez<sup>1</sup>, Patrick Miller<sup>1</sup>, Robert Lee<sup>2</sup>,Paarth Shah<sup>1</sup>,Rares Andrei Ambrus<sup>1</sup>, Haruki Nishimura<sup>1</sup>, Masha Itkina<sup>1</sup>
                    </div>
                    <div class="poster-affiliations">
                        <sup>1</sup>Toyota Research Institute (TRI), <sup>2</sup>Woven by Toyota (WbyT)
                    </div>
                </div>

                <div class="poster">
                    <div class="poster-title" onclick="toggleAbstract(1)">
                        Guiding Task and Motion Planning with Large Language Models
                    </div>
                    <div class="poster-authors">
                        Ilyass Taouil<sup>1</sup>, Michal Ciebelski<sup>1</sup>, Victor Dhedin<sup>1</sup>, Angela Dai<sup>1</sup>, Majid Khadiv<sup>1</sup>
                    </div>
                    <div class="poster-affiliations">
                        <sup>1</sup> Technical University of Munich (TUM)
                    </div>
                </div>

                <div class="poster">
                    <div class="poster-title" onclick="toggleAbstract(2)">
                        Preference-Based Long-Horizon Robotic Stacking with Multimodal Large Language Models
                    </div>
                    <div class="poster-authors">
                        Wanming Yu<sup>1</sup>, Adrian R√∂fer<sup>2</sup>, Abhinav Valada<sup>2</sup>, Sethu Vijayakumar<sup>1</sup>
                    </div>
                    <div class="poster-affiliations">
                        <sup>1</sup>University of Edinburgh, <sup>2</sup>University of Freiburg
                    </div>
                </div>

                <div class="poster">
                    <div class="poster-title" onclick="toggleAbstract(3)">
                        ActiveGrounder: 3D Visual Grounding with Object-Hull-Guided Active Observation
                    </div>
                    <div class="poster-authors">
                        Dasol Hong<sup>1</sup>, Juhye Park<sup>1</sup>, Hyun Myung<sup>1</sup>
                    </div>
                    <div class="poster-affiliations">
                        <sup>1</sup>Urban Robotics Lab, KAIST
                    </div>
                </div>

                <div class="poster">
                    <div class="poster-title" onclick="toggleAbstract(4)">
                        BEAST: Efficient Tokenization of B-Splines Encoded Action Sequences for Imitation Learning
                    </div>
                    <div class="poster-authors">
                        Hongyi Zhou<sup>1</sup>, Weiran Liao<sup>1</sup>, Xi Huang<sup>1</sup>, Yucheng Tang<sup>1</sup>, Fabian Otto<sup>1</sup>, Xiaogang Jia<sup>1</sup>, Xinkai Jiang<sup>1</sup>, Simon Hilber<sup>1</sup>, Ge Li<sup>1</sup>, Qian Wang<sup>1</sup>, √ñmer Erdin√ß<sup>1</sup> Yaƒümurlu, Nils Blank<sup>1</sup>, Moritz Reuss<sup>1</sup>, Rudolf Lioutikov<sup>1</sup>
                    </div>
                    <div class="poster-affiliations">
                        <sup>1</sup>Intuitive Robots Lab, Karlsruhe Institute of Technology, Germany
                    </div>
                </div>

            </div>
        </section>





        <section id="contact" class="section">
            <h2>Contact</h2>
            <div class="contact-card">
                <p>
                    For any inquiries related to the workshop, submissions, or participation,
                    feel free to reach out to us at:
                </p>
                <p>
                    <strong>Email:</strong>
                    <a href="mailto:dionis.totsila@inria.fr">dionis.totsila@inria.fr</a>
                </p>
                <p>
                    We look forward to hearing from you!
                </p>
            </div>
        </section>

        <section class="sponsors-banner">
            <div class="sponsors-container">
                <!-- <p class="sponsors-text">Supported by</p> -->
                <div class="sponsor-logos">
                    <img src="assets/inria_logo.png" alt="Inria Logo" />
                    <img src="assets/kit_logo.png" alt="KIT Logo" />
                    <img src="assets/deepmind_logo.png" alt="Google DeepMind Logo" />
                    <img src="assets/eurobin_logo.png" alt="Eurobin Logo">
                </div>
                <!-- <div class="sponsor-logos">

                </div> -->
            </div>
        </section>

        <!-- <div id="cfp-banner" class="cfp-banner">
            üì¢ Call for Posters is still OPEN (Extended Sumbission Deadline)!
            <a href="#cfp" class="cfp-link">Submit your poster here</a>
            <button id="cfp-close-btn" class="cfp-close-btn" aria-label="Close notification">&times;</button>
        </div> -->
    </main>
    <!-- <footer>
        <p>&copy; 2025 Humanoids Workshop ¬∑ All rights reserved</p>
    </footer> -->

    <script src="script.js"></script>
</body>

</html>